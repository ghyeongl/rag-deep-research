ì•Œê² ìŠµë‹ˆë‹¤! 2025ë…„ì— ë°œí‘œëœ RAG(Retrieval-Augmented Generation) ê´€ë ¨ ë…¼ë¬¸ ì¤‘ì—ì„œ ì¸ìš© ìˆ˜ê°€ ë†’ê³  ì£¼ëª©ë°›ëŠ” ë…¼ë¬¸ì„ ì¡°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤. ì—°êµ¬ ë…¼ë¬¸ë¿ë§Œ ì•„ë‹ˆë¼ ë¦¬ë·° ë…¼ë¬¸ë„ í¬í•¨í•˜ì—¬ ìµœì‹  íŠ¸ë Œë“œì™€ ì—°êµ¬ ë°©í–¥ì„ ë¶„ì„í•œ í›„ ê²°ê³¼ë¥¼ ê³µìœ í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

# 2025ë…„ RAG (Retrieval-Augmented Generation) ë¶„ì•¼ì˜ ì£¼ìš” ë…¼ë¬¸ ë¶„ì„

## ê°œìš” ğŸ“
**Retrieval-Augmented Generation (RAG)**ì€ ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸(LLM)ì— **ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰**í•˜ì—¬ ìƒì„± ê³¼ì •ì— í™œìš©í•¨ìœ¼ë¡œì¨, ìˆœìˆ˜ LLMì˜ í•œê³„ì¸ í™˜ê°(hallucination)ì´ë‚˜ ì§€ì‹ ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê¸°ë²•ì…ë‹ˆ ([[2312.10997] Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997#:~:text=,which%20includes%20the%20retrieval%2C%20the))ã€‘. 2024ë…„ì„ ì „í›„ë¡œ RAGì— ëŒ€í•œ ì—°êµ¬ê°€ í­ë°œì ìœ¼ë¡œ ì¦ê°€í•˜ì—¬ â€œ**RAGì˜ í•´**â€ë¼ê³  ë¶ˆë¦´ ì •ë„ë¡œ ë§ì€ ê´€ì‹¬ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ 2024ë…„ í•œ í•´ì—ëŠ” RAG ê´€ë ¨ í”„ë¦¬í”„ë¦°íŠ¸ê°€ **ì£¼ë‹¹ 10í¸ ì´ìƒ** ë°œí‘œë˜ê¸°ë„ í–ˆìŠµë‹ˆ ([The Rise and Evolution of RAG in 2024 A Year in Review | RAGFlow](https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review#:~:text=2024%20from%20both%20industrial%20and,These%20papers%20primarily%20focus%20on))ã€‘. ì´ëŠ” ìµœì‹  ì •ë³´ ì—…ë°ì´íŠ¸ì™€ ì „ë¬¸ ë¶„ì•¼ ì§€ì‹ í†µí•©ì˜ í•„ìš”ì„±, ê·¸ë¦¬ê³  ìƒì„± ë‚´ìš©ì˜ **ì‚¬ì‹¤ì„± í–¥ìƒ** ìš”êµ¬ì— ë¶€ì‘í•œ ê²°ê³¼ì…ë‹ˆë‹¤.

ìµœê·¼ RAG ì—°êµ¬ì˜ íë¦„ì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì´ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤:

- **LLMì˜ ì‚¬ì‹¤ì„± ë° ì •í™•ë„ í–¥ìƒ:** RAGë¥¼ í†µí•´ ì˜ëª»ëœ ì •ë³´ í˜¼ì…ì„ ì¤„ì´ê³  ì‹ ë¢°ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ë²•ë“¤ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤ (ì˜ˆ: ìê¸°ë°˜ì„± ê¸°ë°˜ RAG, ë…¸ì´ì¦ˆ ì œê±°ìš© ê·¼ê±° ìƒì„± ë“±).
- **íŠ¹ì • ë„ë©”ì¸ ì ìš©:** ì±—ë´‡ì´ë‚˜ ê²€ìƒ‰ì—”ì§„ë¿ë§Œ ì•„ë‹ˆë¼ ì˜ë£Œ, ë²•ë¥ , ê¸ˆìœµ ë“± **íŠ¹ì • ë¶„ì•¼ì— íŠ¹í™”ëœ RAG** ì‘ìš© ì—°êµ¬ë„ ë“±ì¥í•˜ì—¬, ë„ë©”ì¸ ë³„ ì§€ì‹ ê¸°ë°˜ Q&Aì™€ ì „ë¬¸ìë£Œ ê²€ìƒ‰ì— RAGë¥¼ í™œìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- **ì¢…í•©ì ì¸ ë¦¬ë·°ì™€ ë²¤ì¹˜ë§ˆí¬:** ê¸‰ì†í•œ ì—°êµ¬ í™•ì‚°ì— ë”°ë¼ ìµœì‹  RAG ê¸°ë²•ë“¤ì„ ì •ë¦¬í•˜ê³  í‰ê°€í•˜ê¸° ìœ„í•œ **ì„¤ë¬¸(ë¦¬ë·°) ë…¼ë¬¸**ê³¼ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ë„ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì—°êµ¬ìë“¤ì´ RAGì˜ ë°œì „ ë°©í–¥ê³¼ í•œê³„ë¥¼ í•œëˆˆì— íŒŒì•…í•˜ë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.

ì•„ë˜ì—ì„œëŠ” **2024~2025ë…„ì— ë°œí‘œëœ ì£¼ìš” RAG ê´€ë ¨ ë…¼ë¬¸**ë“¤ì„ ì„ ì •í•˜ì—¬, ë°œí‘œ venueì™€ ì¸ìš© ì˜í–¥ë„, í•µì‹¬ ê¸°ì—¬ ë‚´ìš© ë° ì„±ëŠ¥ ì •ë³´ë¥¼ í‘œë¡œ ì •ë¦¬í•˜ê³  ê° ë…¼ë¬¸ì˜ íŠ¹ì§•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ìµœê·¼ RAG ê´€ë ¨ ì£¼ìš” ë…¼ë¬¸ (2024~2025) ğŸ“‘

| ë…¼ë¬¸ (ë°œí‘œ ì—°ë„)                                | ë°œí‘œ ë§¤ì²´                | í•µì‹¬ ê¸°ì—¬ ë° ë‚´ìš©                                                   | ë¹„ê³  (íŠ¹ì§• ë° ì‘ìš© ë¶„ì•¼)                 |
|-----------------------------------------------|-----------------------|-----------------------------------------------------------------|---------------------------------------|
| **Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection**<br>(2024) â€“ *ICLR 2024* | êµ­ì œ í•™íšŒ (ICLR)<br>ğŸ“„ | LLMì´ **í•„ìš”í•  ë•Œë§Œ ê²€ìƒ‰**í•˜ê³ , **ìê¸° ê²€í† (self-reflection)**ë¥¼ í†µí•´ ì‘ë‹µì˜ ì‚¬ì‹¤ì„±ì„ ê²€ì¦í•˜ë„ë¡ í›ˆë ¨í•œ í”„ë ˆì„ì›Œí¬. íŠ¹ìˆ˜ í† í°ì„ ì‚¬ìš©í•´ ëª¨ë¸ ìŠ¤ìŠ¤ë¡œ ê²€ìƒ‰ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ê³ , ê²€ìƒ‰ ê²°ê³¼ì™€ ìì‹ ì˜ ë‹µë³€ì— ëŒ€í•´ ë¹„íŒì ìœ¼ë¡œ í‰ê°€í•˜ë„ë¡ í•¨. | ìµœì‹  GPT(ì˜ˆ: ChatGPT)ë‚˜ ê¸°ì¡´ RAG ê¸°ë²• ëŒ€ë¹„ **ë†’ì€ ì •í™•ë„**ì™€ ì‚¬ì‹¤ì„±ì„ ë³´ì—¬, ì—¬ëŸ¬ ì§€ì‹ íƒìƒ‰ ê³¼ì œì—ì„œ SOTA ([[2310.11511] Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://arxiv.org/abs/2310.11511#:~:text=to%20diverse%20task%20requirements,generations%20relative%20to%20these%20models))71ã€‘. ì½”ë“œ ê³µê°œë¡œ í›„ì† ì—°êµ¬ì— ë§ì´ ì¸ìš©ë¨. |
| **InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales**<br>(2025) â€“ *ICLR 2025* | êµ­ì œ í•™íšŒ (ICLR)<br>ğŸ“„ | LLMì´ ê²€ìƒ‰ ê²°ê³¼ì˜ ë…¸ì´ì¦ˆë¥¼ **ìì²´ì ìœ¼ë¡œ ì œê±°**í•˜ë„ë¡ **ê·¼ê±°(rationale)ë¥¼ ìƒì„±**í•˜ì—¬ í•™ìŠµì‹œí‚¤ëŠ” RAG í”„ë ˆì„ì›Œí¬. ì •ë‹µ ë„ì¶œ ê³¼ì •ì„ ì„¤ëª…í•˜ëŠ” ìê¸°-í•©ì„± ê·¼ê±°ë¥¼ ë§Œë“¤ì–´ ëª¨ë¸ì„ ëª…ì‹œì ìœ¼ë¡œ **ì •ë‹µ ë„ì¶œ ê²½ë¡œ í•™ìŠµ** (denoising ([InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales](https://arxiv.org/html/2406.13629v2#:~:text=involving%20significant%20human%20efforts,outperforms%20existing%20RAG%20methods%20in))L48ã€‘. ì¶”ê°€ ì¸ê°„ ë¼ë²¨ ì—†ì´ë„ ëª¨ë¸ì˜ ì‘ë‹µ ê·¼ê±°ë¥¼ í™•ì¸ ê°€ëŠ¥í•˜ê²Œ í•´ **ì‹ ë¢°ì„±** í–¥ìƒ. | ë‹¤ì–‘í•œ ì§€ì‹ ì§‘ì¤‘ ê³¼ì œ 5ì¢…ì—ì„œ ê¸°ì¡´ RAG ëŒ€ë¹„ **í‰ê·  8.3% ì„±ëŠ¥ í–¥ìƒ** ([InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales](https://arxiv.org/html/2406.13629v2#:~:text=fine,domain%20datasets%2C%20demonstrating%20strong))L53ã€‘. ê²€ìƒ‰ ê²°ê³¼ê°€ ë§ì•„ì ¸ë„ ê°•ê±´í•˜ê²Œ ë…¸ì´ì¦ˆ ì œê±°ë¥¼ ìˆ˜í–‰, ë²”ìš©ì„± ì…ì¦. |
| **Reliability-Aware RAG (RA-RAG)**: *Retrieval-Augmented Generation with Estimation of Source Reliability*<br>(2025) â€“ *ICLR 2025* ì œì¶œ | êµ­ì œ í•™íšŒ (ICLR)<br>ğŸ“„ | ì™¸ë¶€ ì§€ì‹ ì†ŒìŠ¤ë§ˆë‹¤ **ì‹ ë¢°ë„**ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤ëŠ” ì ì— ì£¼ëª©í•˜ì—¬, ë¬¸í—Œ **ì¶œì²˜ì˜ ì‹ ë¢°ë„ë¥¼ ì¶”ì •**í•´ ê²€ìƒ‰ê³¼ ìƒì„± ë‹¨ê³„ì— í™œìš©í•˜ëŠ” RAG ê¸°ë²•. ë¼ë²¨ ì—†ì´ë„ iterativeí•˜ê²Œ ì†ŒìŠ¤ ì‹ ë¢°ë„ì™€ ì°¸ë‹µì„ ì¶”ì •í•˜ê³ , ì‹ ë¢°ë„ê°€ ë†’ì€ ì¶œì²˜ ìœ„ì£¼ë¡œ ê²€ìƒ‰í•´ **ê°€ì§œì •ë³´ ì „íŒŒë¥¼ ([RETRIEVAL-AUGMENTED GENERATION WITH ESTIMATION OF SOURCE RELIABILITY | OpenReview](https://openreview.net/forum?id=J3xRByRqOz#:~:text=propagating%20misinformation,to%20a%20set%20of%20baselines))-L42ã€‘. ê²€ìƒ‰ëœ ì¦ê±°ë“¤ì„ ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ë¡œ ì§‘ê³„í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±. | **ì‚¬ì‹¤ì„± & ì•ˆì „ì„± ê°•í™”**ë¥¼ ëª©í‘œë¡œ í•œ RAGë¡œ, ì‹ ë¢°ë„ ë‹¤ì–‘í•œ í˜„ì‹¤ ì›¹ í™˜ê²½ì— ì í•©. ìì²´ **ë²¤ì¹˜ë§ˆí¬**ë¥¼ êµ¬ì„±í•´ ì‹¤í—˜í•œ ê²°ê³¼ ê¸°ì¡´ RAG ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ê³¼ **ì˜¤ë¥˜ ê°ì†Œ íš¨ê³¼** ([RETRIEVAL-AUGMENTED GENERATION WITH ESTIMATION OF SOURCE RELIABILITY | OpenReview](https://openreview.net/forum?id=J3xRByRqOz#:~:text=propagating%20misinformation,to%20a%20set%20of%20baselines))-L42ã€‘. |
| **CRAG: Corrective Retrieval-Augmented Generation**<br>(2024) â€“ *arXiv preprint* | arXiv ê³µê°œ<br>ğŸ“„ | **ì˜ëª»ëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ êµì •**í•˜ì—¬ ìƒì„± í’ˆì§ˆì„ ë†’ì´ëŠ” RAG í”„ë ˆì„ì›Œí¬. ê²½ëŸ‰ í‰ê°€ëª¨ë¸ë¡œ **ê²€ìƒ‰ í’ˆì§ˆì„ ì ê²€**í•˜ê³  ì‹ ë¢°ë„ì— ë”°ë¼ **ë‹¤ë¥¸ ì§€ì‹ ì†ŒìŠ¤ë¥¼ í™œìš©**í•¨. ì˜ˆë¥¼ ë“¤ì–´ ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì‹¤í•˜ë©´ **ì›¹ ê²€ìƒ‰**ìœ¼ë¡œ ë³´ì™„í•˜ê³ , ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ì—ì„œë„ **ì¤‘ìš” ì •ë³´ë§Œ ë°œì·Œ**í•˜ë„ë¡ ë¶„í•´-ì¬êµ¬ì„± ì•Œ ([[2401.15884] Corrective Retrieval Augmented Generation](https://arxiv.org/abs/2401.15884#:~:text=To%20this%20end%2C%20we%20propose,based))5-L64ã€‘. ëª¨ë“ˆí˜• ì„¤ê³„ë¡œ ê¸°ì¡´ ì–´ë–¤ RAG íŒŒì´í”„ë¼ì¸ì—ë„ ì—°ê²° ê°€ëŠ¥. | ê³ ì •ëœ ì‚¬ì„¤ ì§€ì‹ë² ì´ìŠ¤ì— ì˜ì¡´í•˜ëŠ” RAGì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ì—¬ **ì˜¤í”ˆ ë„ë©”ì¸ ì§ˆì˜ì‘ë‹µ** ë“±ì—ì„œ ê²¬ê³ í•œ ì„±ëŠ¥ ë‹¬ì„±. 4ê°œ ë°ì´í„°ì…‹ ì‹¤í—˜ì—ì„œ **ê¸°ì¡´ RAG ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ ([[2401.15884] Corrective Retrieval Augmented Generation](https://arxiv.org/abs/2401.15884#:~:text=selectively%20focus%20on%20key%20information,based%20approaches))3-L67ã€‘. ì‹¤ìš©ì  RAG ì‹œìŠ¤í…œ êµ¬ì¶•ì— ì˜í–¥. |
| **RAG2: Rationale-Guided Retrieval-Augmented Generation for Medical QA**<br>(2024) â€“ *arXiv preprint* | arXiv ê³µê°œ<br>ğŸ“„ | **ì˜ë£Œ ë¶„ì•¼** ì§ˆì˜ì‘ë‹µì— íŠ¹í™”ëœ RAG í”„ë ˆì„ì›Œí¬ë¡œ, LLMì´ ìƒì„±í•œ **ê·¼ê±° ë¬¸ì¥**ì„ ì§ˆì˜ë¡œ í™œìš©í•˜ê³ , **ì—¬ëŸ¬ ì˜ë£Œ ë°ì´í„°ë² ì´ìŠ¤**ì—ì„œ ê³ ë¥´ê²Œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ë„ë¡ ì„¤ê³„. ë˜í•œ perplexity ê¸°ë°˜ **í•„í„° ëª¨ë¸**ë¡œ ì˜í•™ì ìœ¼ë¡œ ë¬´ê´€í•œ ë¬¸ë§¥ì„ ê±¸ëŸ¬ë‚´ì–´, LLMì— **ìœ ìš©í•œ í•µì‹¬ ë‚´ ([Rationale-Guided Retrieval Augmented Generation for Medical Question Answering](https://arxiv.org/html/2411.00300v1#:~:text=specific%20source%20corpus%20they%20were,Our%20experiments))60-L67ã€‘. | ì˜ë£Œ AI ë¶„ì•¼ì—ì„œ LLMì˜ í™˜ê°ì„ ì¤„ì´ê³  ìµœì‹  ì§€ì‹ì„ ë°˜ì˜. 3ê°œì˜ ì˜ë£Œ QA ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ìµœê³  RAGë³´ë‹¤ **ìµœëŒ€ 5.6% ë†’ì€ ì •í™•ë„**ë¥¼ ê¸°ë¡í•˜ëŠ”  ([Rationale-Guided Retrieval Augmented Generation for Medical Question Answering](https://arxiv.org/html/2411.00300v1#:~:text=biomedical%20corpora%2C%20effectively%20mitigating%20retriever,lab%2FRAG2))67-L71ã€‘. ì˜ë£Œ,ìƒëª…ê³¼í•™ QAì— ì‘ìš©ë¨. |
| **RAG Survey** â€“ *Retrieval-Augmented Generation for LLMs: A Survey*<br>(2024) â€“ *arXiv* | arXiv ê³µê°œ<br>ğŸ“‘ | RAG ì—°êµ¬ì˜ ì „ë°˜ì„ ì•„ìš°ë¥´ëŠ” **ì¢…í•© ì„¤ë¬¸ ë…¼ë¬¸**. RAGì˜ ê¸°ë³¸ êµ¬ì¡°(ê²€ìƒ‰ ë‹¨ê³„, ìƒì„± ë‹¨ê³„, ì¦ê°• ë°©ë²•)ë¥¼ **ì„¸ë¶„í™”í•˜ì—¬ ê¸°ìˆ **í•˜ê³ , ì´ˆê¸° ë‹¨ìˆœ RAGë¶€í„° ë°œì „ëœ RAG, ëª¨ë“ˆí˜• RAGê¹Œì§€ **ë°œì „  ([[2312.10997] Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997#:~:text=detailed%20examination%20of%20the%20progression,avenues%20for%20research%20and%20development))L61-L69ã€‘. ê° êµ¬ì„± ìš”ì†Œë³„ ìµœì²¨ë‹¨ ê¸°ìˆ ì„ ì†Œê°œí•˜ê³ , í‰ê°€ ë°©ë²•ê³¼ ë²¤ì¹˜ë§ˆí¬, ì˜¤í”ˆ ë¬¸ì œ ë° í–¥í›„ ë°©í–¥ê¹Œì§€ ì œì‹œ. | 2023ë…„ê¹Œì§€ì˜ ë°©ëŒ€í•œ RAG ì—°êµ¬ë¥¼ ë§ë¼í•œ **ê°œë¡ ì„œ**ë¡œì„œ, í›„ì† ì—°êµ¬ì—ì„œ ë§ì´ ì¸ìš©. ë‹¤ì–‘í•œ **ì‘ìš© ë„ë©”ì¸(QA, ìš”ì•½ ë“±)**ì—ì„œì˜ RAG í™œìš© ì‚¬ë¡€ì™€ í•œê³„(í™•ì¥ì„±, í¸ ([[2410.12837] A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions](https://arxiv.org/abs/2410.12837#:~:text=language%20models%20to%20enhance%20the,as%20scalability%2C%20bias%2C%20and%20ethical))L55-L63ã€‘. |

## ì„¸ë¶€ ë¶„ì„ ğŸ”

### 1. ìƒˆë¡œìš´ RAG ê¸°ë²• ì œì•ˆ ë…¼ë¬¸ë“¤ ğŸš€
- **Self-RAG (ICLR 2024):** Ak ([[2310.11511] Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://arxiv.org/abs/2310.11511#:~:text=Title%3ASelf,Reflection))0â€ L41-L49ã€‘ì´ ì œì•ˆí•œ **Self-Reflective RAG**ëŠ” LLM ìŠ¤ìŠ¤ë¡œ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì™¸ë¶€ ê²€ìƒ‰ì„ í•˜ê³ , ìƒì„± ë„ì¤‘ì— ìì‹ ì˜ ë‹µë³€ì„ ê²€í† í•˜ì—¬ ê·¼ê±°ë¥¼ ì°¾ëŠ” **ìê¸°ë°˜ì„±í˜• RAG**ì…ë‹ˆë‹¤. ëª¨ë¸ì€ **íŠ¹ìˆ˜ í† í°**ì„ í™œìš©í•´ **â€œì§€ê¸ˆ ê²€ìƒ‰ì´ í•„ìš”í•œì§€â€**ë¥¼ ê²°ì •í•˜ê³ , **ê²€ìƒ‰ ê²°ê³¼ì— ê·¼ê±°í•œ ë¹„íŒì  í‰ê°€**(reflection)ë¥¼ ë‹¨ ([[2310.11511] Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://arxiv.org/abs/2310.11511#:~:text=can%20lead%20to%20unhelpful%20response,art%20LLMs%20and%20retrieval))0â€ L59-L67ã€‘. ì´ë¥¼ í†µí•´ ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ìœ¼ë¡œ ì¸í•œ ì¡ìŒ íˆ¬ì…ì„ ì¤„ì´ê³ , ê²€ìƒ‰í•œ ì •ë³´ë¥¼ ì œëŒ€ë¡œ í™œìš©í•˜ì—¬ **ì‚¬ì‹¤ì ì¸ ì‘ë‹µ**ì„ ë§Œë“­ë‹ˆë‹¤. Self-RAGë¥¼ ì†Œí˜• LLM(7B, 13B)ì— ì ìš©í•œ ì‹¤í—˜ì—ì„œ, **ChatGPT ê°™ì€ ëŒ€í˜• ëª¨ë¸**ì´ë‚˜ ê¸°ì¡´ RAGë³´ë‹¤ë„ **ë†’ì€ ì •í™•ë„**ë¥¼ ë³´ì˜€ìœ¼ë©°, ì˜¤í”ˆë„ë©”ì¸ QA, ì¶”ë¡ , íŒ©íŠ¸ ê²€ì¦ ë“± ë‹¤ì–‘í•œ ê³¼ì œì—ì„œ **ìµœì‹  SOTA ì„±ëŠ¥ ([[2310.11511] Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://arxiv.org/abs/2310.11511#:~:text=to%20diverse%20task%20requirements,generations%20relative%20to%20these%20models))0â€ L66-L71ã€‘. ì´ ì—°êµ¬ëŠ” **ì½”ë“œë¥¼ ê³µê°œ**í•˜ì—¬ ë§ì€ í›„ì† ì—°êµ¬ì—ì„œ ê¸°ë°˜ ê¸°ë²•ìœ¼ë¡œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.

- **InstructRAG (ICLR 2025):** Zh ([InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales](https://arxiv.org/html/2406.13629v2#:~:text=Retrieval,work%2C%20we%20propose%20InstructRAG%2C%20where))4â€ L32-L40ã€‘ì´ ë°œí‘œí•œ InstructRAGëŠ”, **ê²€ìƒ‰ëœ ì •ë³´ì— ì„ì¸ ì˜¤ë¥˜ë‚˜ ë…¸ì´ì¦ˆë¥¼ ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ê±¸ëŸ¬ë‚´ë„ë¡** LMì„ í›ˆë ¨ì‹œí‚¤ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì…ë‹ˆë‹¤. í•µì‹¬ ì•„ì´ë””ì–´ëŠ” **ëª¨ë¸ì—ê²Œ ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ì •ë‹µì„ ë„ì¶œí•˜ëŠ” ê³¼ì •ì„ â€œì„¤ëª…â€í•˜ê²Œ í•¨ìœ¼ë¡œì¨**, ê·¸ ì„¤ëª…ë¬¸(= **ìì²´ ìƒì„±í•œ ê·¼ê±°**)ì„ **ë…¸ì´ì¦ˆ ì œê±°ë¥¼ ìœ„í•œ ì§€ë„ ì‹ í˜¸**ë¡œ ([InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales](https://arxiv.org/html/2406.13629v2#:~:text=other%20hand%2C%20the%20acquisition%20of,and%20effectively%20improves%20generation%20accuracy))4â€ L39-L47ã€‘. êµ¬ì²´ì ìœ¼ë¡œ, ê²€ìƒ‰ ê²°ê³¼ì™€ ì •ë‹µì„ ì¤¬ì„ ë•Œ **ëª¨ë¸ì´ ì •ë‹µê¹Œì§€ ì´ë¥´ëŠ” ì¶”ë¡  ê³¼ì •ì„ ì„œìˆ **í•˜ë„ë¡ í•˜ê³ , ì´ë ‡ê²Œ ì–»ì€ **ìê¸° ê·¼ê±°(rationale)**ë¥¼ ë‹¤ì‹œ ëª¨ë¸ í•™ìŠµ(ë˜ëŠ” few-shot ì‹œ ([InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales](https://arxiv.org/html/2406.13629v2#:~:text=involving%20significant%20human%20efforts,outperforms%20existing%20RAG%20methods%20in))4â€ L40-L48ã€‘. ì¶”ê°€ ì¸ê°„ ë…¸ë ¥ì´ ë“¤ì§€ ì•Šê³ ë„ ëª¨ë¸ì´ **ìŠ¤ìŠ¤ë¡œ ê·¼ê±°ë¥¼ ìƒì„±**í•˜ë¯€ë¡œ **íˆ¬ëª…ì„±**ì´ ë†’ì•„ì§€ê³ , ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì˜ **ì •í™•ë„ í–¥ìƒ**ê³¼ ì˜¤ë¥˜ ([InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales](https://arxiv.org/html/2406.13629v2#:~:text=InstructRAG%20requires%20no%20additional%20supervision%2C,achieving%20a%20relative%20improvement%20of))8â€ L19-L27ã€‘. 5ê°œì— ì´ë¥´ëŠ” ì§€ì‹ ì§‘ì¤‘í˜• ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì—ì„œ InstructRAGëŠ” **ê¸°ì¡´ ëª¨ë“  RAG ê¸°ë²• ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥**ì„ ë‚˜íƒ€ëƒˆê³ , **í‰ê·  8.3%ì˜ ìƒëŒ€ì  ì„±ëŠ¥ í–¥ìƒ* ([InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales](https://arxiv.org/html/2406.13629v2#:~:text=fine,domain%20datasets%2C%20demonstrating%20strong))4â€ L45-L53ã€‘. ë˜í•œ ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜ê°€ ë§ì•„ì ¸ë„ ì¼ê´€ë˜ê²Œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ì—¬, **ë„ë©”ì¸ ë³€í™”ì—ë„ ê°•ê±´í•œ** ìƒì„± í’ˆì§ˆì„  ([InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales](https://arxiv.org/html/2406.13629v2#:~:text=benchmarks,Our%20code%20is%20available%20at)) ([InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales](https://arxiv.org/html/2406.13629v2#:~:text=The%20main%20contributions%20of%20this,synthesis%20method%20that%20does%20not))8â€ L49-L57ã€‘.

- **Reliability-Aware RAG (RA-RAG, ICLR 2025):** Jeongye ([RETRIEVAL-AUGMENTED GENERATION WITH ESTIMATION OF SOURCE RELIABILITY | OpenReview](https://openreview.net/forum?id=J3xRByRqOz#:~:text=Jeongyeon%20Hwang%20%2C%20%203%2C,5%2C%20Jungseul%20Ok))2â€ L15-L23ã€‘ì˜ ì—°êµ¬ëŠ” **â€œê²€ìƒ‰ ì†ŒìŠ¤ì˜ ì‹ ë¢°ë„â€**ì— ì´ˆì ì„ ë§ì¶˜ ë…íŠ¹í•œ ì ‘ê·¼ì…ë‹ˆë‹¤. ì¼ë°˜ì ì¸ RAGëŠ” **ë¬¸ì„œì˜ ê´€ë ¨ì„±**ë§Œ ê³ ë ¤í•´ ê²€ìƒ‰í•˜ëŠ”ë°, ì´ ë…¼ë¬¸ì€ ì‹¤ì œ ì›¹ì—ëŠ” ì¶œì²˜ë§ˆë‹¤ ì‹ ë¢°ë„ê°€ ë‹¤ë¥´ë‹¤ëŠ” ì ì„ ì§€ì í•©ë‹ˆë‹¤. **RA-RAG**ëŠ” **ë‹¤ì¤‘ ì†ŒìŠ¤ í™˜ê²½ì—ì„œ ê° ì¶œì²˜ì˜ ì‹ ë¢°ë„ë¥¼ ì¶”ì •**í•˜ì—¬, ê²€ìƒ‰ ë‹¨ê³„ì—ì„œ **ì‹ ë¢°ë„ ë†’ì€ ì¶œì²˜ ìœ„ì£¼ë¡œ ë¬¸ì„œë¥¼ ì„ íƒ**í•˜ê³ , ìƒì„± ë‹¨ê³„ì—ì„œë„ **ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜**ë¥¼ ë¶€ì—¬í•´ ([RETRIEVAL-AUGMENTED GENERATION WITH ESTIMATION OF SOURCE RELIABILITY | OpenReview](https://openreview.net/forum?id=J3xRByRqOz#:~:text=methods%20often%20overlook%20the%20heterogeneous,scalability%20while%20not%20compromising%20the))2â€ L31-L39ã€‘. ë¼ë²¨ëœ ë°ì´í„° ì—†ì´ë„ iterativeí•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ **ì¶œì²˜ ì‹ ë¢°ë„ì™€ ì ì • ì •ë‹µì„ êµì°¨ ì¶”ì •**í•˜ê³ , ìµœì¢…ì ìœ¼ë¡œ **ê±°ì§“ì •ë³´ë¥¼ ë°°ì œí•œ ì •í™•í•œ ì‘ë‹µ**ì„ ì–»ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤. ì €ìë“¤ì€ **ì´ì§ˆì  ì‹ ë¢°ë„ì˜ ì¶œì²˜ë“¤ì´ í˜¼ì¬ëœ í˜„ì‹¤ ì‹œë‚˜ë¦¬ì˜¤**ë¥¼ ë°˜ì˜í•œ ìƒˆë¡œìš´ í‰ê°€ ë°ì´í„°ë¥¼ êµ¬ì¶•í•˜ê³  RA-RAGë¥¼ ê²€ì¦í•œ ê²°ê³¼, **ê¸°ì¡´ RAG ëŒ€ë¹„ ë›°ì–´ë‚œ íš¨ê³¼**ë¥¼  ([RETRIEVAL-AUGMENTED GENERATION WITH ESTIMATION OF SOURCE RELIABILITY | OpenReview](https://openreview.net/forum?id=J3xRByRqOz#:~:text=with%20no%20labelling,to%20a%20set%20of%20baselines))2â€ L37-L42ã€‘. ì´ë¥¼ í†µí•´ ì‚¬ì‹¤ í™•ì¸ì´ ì¤‘ìš”í•œ ë‰´ìŠ¤, ì§€ì‹ ê·¸ë˜í”„, ì›¹ ê²€ìƒ‰ ë“±ì—ì„œ **ì‹ ë¢°ì„± ìˆëŠ” RAG**ì˜ ê°€ëŠ¥ì„±ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.

- **CRAG (Corrective RAG, 2024):** Sh ([[2401.15884] Corrective Retrieval Augmented Generation](https://arxiv.org/abs/2401.15884#:~:text=,this%20version%2C%20v3))1â€ L39-L47ã€‘ì´ ë°œí‘œí•œ CRAGëŠ” RAG íŒŒì´í”„ë¼ì¸ì— **â€œì˜¤ë¥˜ ê°ì§€ ë° êµì •â€** ë‹¨ê³„ë¥¼ ì¶”ê°€í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. LLMì´ ì˜ëª»ëœ ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” ì›ì¸ ì¤‘ í•˜ë‚˜ì¸ **ë¶€ì ì ˆí•œ ê²€ìƒ‰ê²°ê³¼**ì— ëŒ€ì‘í•˜ì—¬, CRAGëŠ” **ê²½ëŸ‰ í‰ê°€ ëª¨ë¸**ë¡œ í˜„ì¬ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ **ì „ë°˜ì ì¸ í’ˆì§ˆì„ í‰ê°€**í•˜ê³  **ì‹ ë¢°ë„ ì  ([[2401.15884] Corrective Retrieval Augmented Generation](https://arxiv.org/abs/2401.15884#:~:text=To%20this%20end%2C%20we%20propose,Besides%2C%20a))1â€ L55-L61ã€‘. ì´ ì ìˆ˜ê°€ ë‚®ì„ ê²½ìš° **ëŒ€ì²´ ì§€ì‹ ì†ŒìŠ¤**ë¥¼ í™œìš©í•˜ëŠ”ë°, ì˜ˆë¥¼ ë“¤ì–´ ë‚´ë¶€ ì§€ì‹ë² ì´ìŠ¤ê°€ ë¶€ì¡±í•œ ì •ë³´ëŠ” **ëŒ€ê·œëª¨ ì›¹ ê²€ìƒ‰**ì„ í†µí•´ ì¶”ê°€ë¡œ ([[2401.15884] Corrective Retrieval Augmented Generation](https://arxiv.org/abs/2401.15884#:~:text=evaluator%20is%20designed%20to%20assess,based))1â€ L57-L64ã€‘. ë˜í•œ ê°€ì ¸ì˜¨ ë¬¸ì„œë“¤ë„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , **â€œë¶„í•´ í›„ ì¬êµ¬ì„±â€ ì•Œê³ ë¦¬ì¦˜**ìœ¼ë¡œ **í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ**í•˜ê³  ë¶ˆí•„ìš”í•œ  ([[2401.15884] Corrective Retrieval Augmented Generation](https://arxiv.org/abs/2401.15884#:~:text=retrieval%20actions%20can%20be%20triggered,based))1â€ L59-L64ã€‘. ì´ë ‡ê²Œ í•˜ë©´ ë…¸ì´ì¦ˆë¥¼ ìµœì†Œí™”í•˜ë©´ì„œë„ í•„ìš”í•œ ì™¸ë¶€ì§€ì‹ì„ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. CRAGëŠ” ë³„ë„ì˜ ë³µì¡í•œ ëª¨ë¸ êµì²´ ì—†ì´ë„ **ê¸°ì¡´ ì–´ë–¤ RAG ë°©ì‹ì—ë„ í”ŒëŸ¬ê·¸ì¸**ì²˜ëŸ¼ ì ìš©í•  ìˆ˜ ìˆëŠ” ê²ƒì´ ì¥ì ì´ë©°, ì‹¤ì œ ìš”ì•½/ìƒì„± ë°ì´í„°ì…‹ 4ì¢… ì‹¤í—˜ì—ì„œ ì›ë˜ RAG ëŒ€ë¹„ **í˜„ì €í•œ ì„±ëŠ¥ í–¥ìƒ ([[2401.15884] Corrective Retrieval Augmented Generation](https://arxiv.org/abs/2401.15884#:~:text=selectively%20focus%20on%20key%20information,based%20approaches))1â€ L63-L67ã€‘. ì¦‰, **ê²€ìƒ‰ ë‹¨ê³„ì˜ ì˜¤ë¥˜ë¥¼ ë™ì ìœ¼ë¡œ ë³´ì™„**í•¨ìœ¼ë¡œì¨ RAG ì‹œìŠ¤í…œì˜ ì•ˆì •ì„±ê³¼ ì •í™•ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ ì—°êµ¬ì…ë‹ˆë‹¤.

### 2. íŠ¹ì • ì‘ìš© ë¶„ì•¼ì—ì„œì˜ RAG í™œìš© ğŸ’¡
RAGëŠ” ì§€ì‹ì´ ë¹ ë¥´ê²Œ ë³€í•˜ê±°ë‚˜ ì •í™•í•œ ì¶œì²˜ ê·¼ê±°ê°€ ì¤‘ìš”í•œ **ì „ë¬¸ ë„ë©”ì¸**ì—ì„œë„ í™œë°œíˆ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. RAGë¥¼ ë„ì…í•˜ë©´ LLMì´ ìµœì‹  ì „ë¬¸ ì •ë³´ë¥¼ ì°¸ì¡°í•˜ì—¬ ë‹µë³€í•¨ìœ¼ë¡œì¨, **ê³ ë„ì˜ ì •í™•ì„±ê³¼ ì‹ ë¢°ì„±**ì´ ìš”êµ¬ë˜ëŠ” ë¶„ì•¼ì—ì„œë„ LLMì˜ ([InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales](https://arxiv.org/html/2406.13629v2#:~:text=While%20large%20language%20models%20,76%20%2C%20%2035%2C%2094))44â€ L59-L66ã€‘. ì•„ë˜ëŠ” **ì˜ë£Œ**ì™€ **ë²•ë¥ ** ë¶„ì•¼ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ RAG ì ìš© ì—°êµ¬ë¥¼ ì‚´í´ë³¸ ê²ƒì…ë‹ˆë‹¤:

- **ì˜ë£Œ ë¶„ì•¼ â€“ RAG2 (2024 ([Rationale-Guided Retrieval Augmented Generation for Medical Question Answering](https://arxiv.org/html/2411.00300v1#:~:text=Rationale,Medical%20Question%20Answering))29â€ L43-L51ã€‘ì˜ *RAG2*ëŠ” ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì„ **ì˜ë£Œ QA**ì— ì ìš©í•  ë•Œì˜ í•œê³„ë¥¼ ê°œì„ í•œ ì—°êµ¬ì…ë‹ˆë‹¤. ì˜ë£Œ ë„ë©”ì¸ì—ì„œëŠ” LLMì´ ì¼ë°˜ ì§€ì‹ì— ì—†ëŠ” ìµœì‹  ì˜í•™ ì •ë³´ë¥¼ í•„ìš”ë¡œ í•˜ê±°ë‚˜, ì˜ëª»ëœ ë§¥ë½ì´ íˆ¬ì…ë  ê²½ìš° ìœ„í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. RAG2ëŠ” ìš°ì„  LLMì´ **ìŠ¤ìŠ¤ë¡œ ì˜ë£Œ ì§ˆë¬¸ì— ëŒ€í•œ ì´ˆê¸° í•´ì„¤(ê·¼ê±° ë¬¸ì¥)**ì„ ìƒì„±í•˜ë©´, ì´ë¥¼ **ìƒˆë¡œìš´ ì§ˆì˜ë¡œ í™œìš©**í•˜ì—¬ ì˜ë£Œ ì „ë¬¸ ë¬¸í—Œì—ì„œ **ê´€ë ¨ ì • ([Rationale-Guided Retrieval Augmented Generation for Medical Question Answering](https://arxiv.org/html/2411.00300v1#:~:text=for%20helpful%20information%2C%20and%20,Our%20experiments))29â€ L59-L67ã€‘. ë˜í•œ í•œ ê°€ì§€ ë°ì´í„° ì†ŒìŠ¤ì— ì¹˜ìš°ì¹˜ì§€ ì•Šê³  **ì˜ë£Œ ë…¼ë¬¸, ì„ìƒ ì§€ì¹¨, ì˜ì•½í’ˆ DB ë“± 4ê°€ì§€ ì¶œì²˜**ì—ì„œ ê³¨ê³ ë£¨ ì¦ê±°ë¥¼ ì°¾ë„ë¡ í•´ **í¸í–¥ì„ ì¤„ì˜€ê³ **, ê²€ìƒ‰ëœ ë¬¸ë§¥ ì¤‘ **ëª¨ë¸ì´ ì°¸ê³ í•  ê°€ì¹˜ê°€ ë†’ì€ ë¶€ë¶„ë§Œ ë‚¨ê¸°ëŠ” í•„ ([Rationale-Guided Retrieval Augmented Generation for Medical Question Answering](https://arxiv.org/html/2411.00300v1#:~:text=specific%20source%20corpus%20they%20were,Our%20experiments))29â€ L60-L67ã€‘. ì´ë ‡ê²Œ í•´ì„œ LLMì´ **ë¯¿ì„ ë§Œí•œ ì˜í•™ ì§€ì‹**ì„ ì¶©ë¶„íˆ ì–»ì€ í›„ ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, RAG2ëŠ” ì˜ë£Œ ì‹œí—˜ ë¬¸ì œ(MMLU-Med ë“±)ì™€ ì˜í•™ ì§ˆì˜ì‘ë‹µ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ RAG ê¸°ë°˜ ëª¨ë¸ë“¤(ì˜ˆ: Almanac ë“±)ë³´ë‹¤ **ë†’ì€ ì •í™•ë„(+5~6%)**ë¥¼ ê¸°ë¡í•˜ë©° ìƒˆë¡œìš´ ìƒíƒœ-of-the ([Rationale-Guided Retrieval Augmented Generation for Medical Question Answering](https://arxiv.org/html/2411.00300v1#:~:text=biomedical%20corpora%2C%20effectively%20mitigating%20retriever,lab%2FRAG2))29â€ L67-L71ã€‘. ì´ ì—°êµ¬ëŠ” ì˜ë£Œ AI ì±—ë´‡ì´ë‚˜ ì„ìƒ ì˜ì‚¬ê²°ì • ì§€ì› ì‹œìŠ¤í…œì— RAGë¥¼ í™œìš©í•˜ëŠ” ì¢‹ì€ ì˜ˆì‹œë¡œ í‰ê°€ë©ë‹ˆë‹¤.

- **ë²•ë¥  ë¶„ì•¼ â€“ LegalBench-RAG (2024):** ë²•ë¥  domainì—ì„œë„ RAGì˜ í™œìš©ì„ ì¸¡ì •í•˜ê¸° ìœ„í•œ ì‹œë„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. Lee ë“±ì€ **LegalBench-RAG**ë¼ëŠ” **ë²•ë¥ íŠ¹í™” ë²¤ì¹˜ë§ˆí¬**ë¥¼ êµ¬ì¶•í•˜ì—¬, ë²•ë¥  ì§ˆì˜ì— ëŒ€í•œ RAGì˜ ê²€ìƒ‰ ([LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain](https://arxiv.org/html/2408.10343#:~:text=and%20are%20becoming%20increasingly%20relevant,context%20windows%20cost%20more%20to))25â€ L43-L52ã€‘. ì¼ë°˜ì ì¸ ë²•ë¥  AI ë²¤ì¹˜ë§ˆí¬ê°€ LLMì˜ íŒë¡€ í•™ìŠµ ì •ë„ë‚˜ ì¶”ë¡  ëŠ¥ë ¥ì„ ë³´ëŠ” ë° ë°˜í•´, LegalBench-RAGëŠ” **ëŒ€í˜• ë²•ë¥  ë§ë­‰ì¹˜ì—ì„œ ê´€ë ¨ ì¡°ë¬¸ì´ë‚˜ íŒë¡€ì˜ â€œì •í™•í•œ ì¡°ê°â€ì„ ì°¾ì•„ë‚´ëŠ”  ([LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain](https://arxiv.org/html/2408.10343#:~:text=Large%20Language%20Models%20,LLMs%20to%20generate%20citations%20for))25â€ L45-L54ã€‘. ë§¥ë½ ì°½ í•œê³„ë¥¼ ê³ ë ¤í•´ **ì§§ì§€ë§Œ í•µì‹¬ì ì¸ ë²•ë¥  ì¡°ê°**ì„ ì°¾ëŠ” ê²ƒì´ ëª©í‘œì´ë©°, ì´ë ‡ê²Œ ì •í™•íˆ ì°¾ì•„ë‚¸ ê·¼ê±°ë¥¼ í†µí•´ LLMì´ ë‹µë³€í•˜ë©´ **ì¶œì²˜ê°€ ëª…í™•í•œ ë²•ë¥   ([LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain](https://arxiv.org/html/2408.10343#:~:text=introduce%20LegalBench,is%20constructed%20by%20retracing%20the))25â€ L47-L55ã€‘. ì´ ë²¤ì¹˜ë§ˆí¬ì˜ ê³µê°œë¡œ, í–¥í›„ ë²•ë¥  ì±—ë´‡ì´ë‚˜ íŒë¡€ ê²€ìƒ‰ ì—”ì§„ì— RAGë¥¼ ë„ì…í•  ë•Œ **ê°ê´€ì ì¸ í‰ê°€ ê¸°ì¤€**ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ë²•ë¥  ì™¸ì—ë„ **ê¸ˆìœµ, í•™ìˆ ** ë“± ì „ë¬¸ ë¶„ì•¼ì—ì„œë„ RAGë¥¼ í™œìš©í•œ íŠ¹í™” ì‹œìŠ¤í…œ ê°œë°œê³¼ ê·¸ í‰ê°€ì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤.

### 3. RAGì— ëŒ€í•œ ë¦¬ë·° ë° ë™í–¥ ë¶„ì„ ğŸ—‚
RAG ì—°êµ¬ì˜ í­ë°œì ì¸ ì¦ê°€ëŠ” ì—¬ëŸ¬ **ì¢…í•© ë¦¬ë·° ë…¼ë¬¸**ì˜ ë“±ì¥ìœ¼ë¡œë„ ì´ì–´ì¡ŒìŠµë‹ˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ Gao ë“±(2024)ì€ **â€œRetrieval-Augmented Generation for Large Language Models: A Surveyâ€**ë¥¼ í†µí•´ ì§€ê¸ˆê¹Œì§€ì˜ RAG ì—°êµ¬ë¥¼  ([[2312.10997] Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997#:~:text=detailed%20examination%20of%20the%20progression,avenues%20for%20research%20and%20development))ë‹¤. ì´ ì„¤ë¬¸ ë…¼ë¬¸ì—ì„œëŠ” RAGì˜ **ê°œë…ê³¼ êµ¬ì¡°**ë¥¼ ì •ë¦¬í•˜ê³ , **Naive RAG â†’ Advanced RAG â†’ Modular RAG**ë¡œ ì´ì–´ì§€ëŠ” **ë°œì „ ë‹¨ê³„**ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤. ë˜í•œ **ê²€ìƒ‰ê¸°, ìƒì„±ê¸°, ì¦ê°• ëª¨ë“ˆ**ë¡œ ë‚˜ëˆ„ì–´ ê° ë¶€ë¶„ë³„ ìµœì‹  ê¸°ë²•ë“¤ì„ ì†Œê°œí•˜ê³  ì¥ë‹¨ì ì„ ë¹„êµí–ˆìŠµë‹ˆë‹¤. ìµœì‹  ë²¤ì¹˜ë§ˆí¬ì™€ í‰ê°€ ë°©ë²•ë„ ì •ë¦¬í•˜ì—¬ ì—°êµ¬ìë“¤ì´ **ê°ê´€ì ìœ¼ë¡œ RAG ëª¨ë¸ì„ í‰ê°€**í•  ìˆ˜ ìˆë„ë¡ ë„ì™”ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ RAGê°€ ì§ë©´í•œ **ê³¼ì œ(ì˜ˆ: í™•ì¥ì„±, í¸í–¥, ìœ¤ë¦¬ ì´ìŠˆ)**ì™€ **í–¥í›„ ì—°êµ¬ ë°©í–¥ ([[2410.12837] A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions](https://arxiv.org/abs/2410.12837#:~:text=language%20models%20to%20enhance%20the,as%20scalability%2C%20bias%2C%20and%20ethical)) ([[2312.10997] Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997#:~:text=tripartite%20foundation%20of%20RAG%20frameworks%2C,avenues%20for%20research%20and%20development))ã€‘. ì´ ë°–ì—ë„ G ([[2410.12837] A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions](https://arxiv.org/abs/2410.12837#:~:text=,answering%2C%20summarization)))ì˜ ì„¤ë¬¸ì€ RAGì˜ **ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼(QA, ìš”ì•½, ì§€ì‹ ê¸°ë°˜ ëŒ€í™”)**ì—ì„œì˜ í™œìš©ê³¼ í•œê³„ë¥¼ ë‹¤ë£¨ëŠ” ë“±, ì—¬ëŸ¬ ë¦¬ë·° ë…¼ë¬¸ë“¤ì´ ë°œí‘œë˜ì–´ ì—°êµ¬ìì™€ ì‚°ì—… ì¢…ì‚¬ìë“¤ì—ê²Œ **í•™ìŠµ ê°€ì´ë“œë¼ì¸**ì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.

ä»¥ä¸Šì™€ ê°™ì´, 2024~2025ë…„ì˜ RAG ê´€ë ¨ ì—°êµ¬ëŠ” **ê¸°ìˆ ì  í˜ì‹ **(ì˜ˆ: ìê¸°ë°˜ì„±, ë…¸ì´ì¦ˆ ì œê±°, ì‹ ë¢°ë„ í‰ê°€)ê³¼ **ì‘ìš©ì˜ í™•ì¥**(ì „ë¬¸ ë„ë©”ì¸ ì ìš©), ê·¸ë¦¬ê³  **ì§€ì‹ì˜ ì¶•ì **(ì¢…í•© ë¦¬ë·°) ì¸¡ë©´ì—ì„œ í™œë°œí•˜ê²Œ ì „ê°œë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í•«í•œ ë…¼ë¬¸ë“¤ì€ ë†’ì€ ì¸ìš© ìˆ˜ë¡œ ê·¸ ì˜í–¥ë ¥ì„ ì¸ì •ë°›ê³  ìˆìœ¼ë©°, RAGê°€ **LLMì˜ í•œê³„ë¥¼ ê·¹ë³µ**í•˜ê³  ë‹¤ì–‘í•œ ì‹¤ì œ ë¬¸ì œì— ì ìš©ë˜ëŠ” ê°€ëŠ¥ì„±ì„ í¬ê²Œ ë†’ì´ê³  ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì•ìœ¼ë¡œë„ RAG ë¶„ì•¼ëŠ” **LLMì˜ ì‚¬ì‹¤ì„± ê°•í™”**ì™€ **ì‹¤ì„¸ê³„ ì§€ì‹ í†µí•©**ì˜ í•µì‹¬ìœ¼ë¡œì„œ ì¤‘ìš”í•œ ì—°êµ¬ ì£¼ì œê°€ ë  ([InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales](https://arxiv.org/html/2406.13629v2#:~:text=While%20large%20language%20models%20,76%20%2C%20%2035%2C%2094)) ([The Rise and Evolution of RAG in 2024 A Year in Review | RAGFlow](https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review#:~:text=preprints%20on%20the%20topic%20of,These%20papers%20primarily%20focus%20on))



# Notable Retrieval-Augmented Generation Papers in 2025

Retrieval-Augmented Generation (RAG) continues to advance rapidly in 2025. Below is a compiled list of **influential RAG-related papers published (or preprinted) around 2025** that are gaining academic attention. These works span top conferences (NeurIPS, ACL/NAACL, ICML, etc.) and platforms (arXiv), and include new methods, surveys, benchmarks, and domain-specific applications. Each entry highlights the paperâ€™s venue/type, key contributions, and notable findings (including any performance comparisons). 

| **Paper (Year, Venue/Type)**                                            | **Key Contributions & Findings**                                                                                                                                                                                                       |
|-------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **GraphRAG: Retrieval-Augmented Generation with Graphs**<br>(2025, *Survey*) | Provides the **first comprehensive survey of graph-integrated RAG** techniques ([[2501.00309] Retrieval-Augmented Generation with Graphs (GraphRAG)](https://ar5iv.org/pdf/2501.00309?ref=hub.athina.ai#:~:text=Given%20the%20broad%20applicability%2C%20the,Our%20survey%20repository%20is)). Proposes a *holistic GraphRAG framework* defining key components (query processor, retriever, organizer, generator, data source) and reviews how leveraging graph-structured knowledge improves RAG across domains. Discusses unique challenges of graphs (heterogeneous, relational data) and outlines future research directions ([[2501.00309] Retrieval-Augmented Generation with Graphs (GraphRAG)](https://ar5iv.org/pdf/2501.00309?ref=hub.athina.ai#:~:text=Given%20the%20broad%20applicability%2C%20the,Our%20survey%20repository%20is)). *Significance:* Establishes a unified view of â€œGraphRAG,â€ expected to guide further research in using knowledge graphs to reduce hallucinations and enhance reasoning in generation. |
| **MiniRAG: Towards Extremely Simple RAG**<br>(2025, *Method*)           | Introduces **MiniRAG**, a lightweight RAG system for **small language models (SLMs)** in resource-constrained settings ([[2501.06713] MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation](https://ar5iv.org/pdf/2501.06713?ref=hub.athina.ai#:~:text=efficiency,evaluating%20lightweight%20RAG%20systems%20under)). Key innovations: (1) a *semantic-aware heterogeneous graph index* that stores text chunks and named entities together (reducing reliance on deep semantic understanding), and (2) a *topology-enhanced retrieval* that uses graph structure for efficient knowledge lookup ([[2501.06713] MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation](https://ar5iv.org/pdf/2501.06713?ref=hub.athina.ai#:~:text=efficiency,evaluating%20lightweight%20RAG%20systems%20under)). **Results:** Achieves performance **comparable to LLM-based RAG** methods using only a 7B parameter model, while requiring just ~25% of the storage ([[2501.06713] MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation](https://ar5iv.org/pdf/2501.06713?ref=hub.athina.ai#:~:text=efficiency,evaluating%20lightweight%20RAG%20systems%20under)). This enables on-device RAG deployment without large models, expanding accessibility. |
| **VideoRAG: RAG over Video Corpus**<br>(2025, *Method*)                | Proposes **VideoRAG**, a framework extending RAG to use video content as a knowledge source ([[2501.05874] VideoRAG: Retrieval-Augmented Generation over Video Corpus](https://ar5iv.org/pdf/2501.05874?ref=hub.athina.ai#:~:text=richness,that%20it%20is%20superior%20to)). It dynamically **retrieves relevant video clips** based on a query and incorporates both visual frames and associated text in answer generation ([[2501.05874] VideoRAG: Retrieval-Augmented Generation over Video Corpus](https://ar5iv.org/pdf/2501.05874?ref=hub.athina.ai#:~:text=richness,that%20it%20is%20superior%20to)). Built on Large Video-Language Models (LVLMs) to process and represent video content, it **outperforms text-only or image-only baselines** on knowledge-intensive tasks ([[2501.05874] VideoRAG: Retrieval-Augmented Generation over Video Corpus](https://ar5iv.org/pdf/2501.05874?ref=hub.athina.ai#:~:text=richness,that%20it%20is%20superior%20to)). *Impact:* Unlocks multimodal knowledge for RAG, improving factual accuracy and context in domains like education, how-to guidance, and situational decision-making (where videos provide richer info than text). |
| **SafeRAG: Benchmarking Security in RAG**<br>(2025, *Benchmark/Evaluation*) | Introduces **SafeRAG**, the first security evaluation suite for RAG systems ([[2501.18636] SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model](https://ar5iv.org/pdf/2501.18636?ref=hub.athina.ai#:~:text=security,degradation%20of%20RAG%20service%20quality)). Defines **four adversarial attack types** against RAG pipelines â€“ *â€œsilver noiseâ€*, *â€œinter-context conflictâ€*, *â€œsoft advertisementâ€*, and *â€œwhite Denial-of-Serviceâ€* â€“ and constructs a curated dataset to test each ([[2501.18636] SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model](https://ar5iv.org/pdf/2501.18636?ref=hub.athina.ai#:~:text=security,degradation%20of%20RAG%20service%20quality)). Evaluates 14 representative RAG components (retrievers, rerankers, generators, etc.) under attack; **findings:** current RAG pipelines are *highly vulnerable* â€“ even simple injected false content can bypass filters and cause severe answer manipulation ([[2501.18636] SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model](https://ar5iv.org/pdf/2501.18636?ref=hub.athina.ai#:~:text=security,degradation%20of%20RAG%20service%20quality)). *Significance:* Highlights critical security risks in RAG (e.g. potential for misinformation via poisoned corpora), urging development of robust defenses. |
| **Agentic RAG: A Survey on Agentic Retrieval-Augmented Generation**<br>(2025, *Survey*) | Surveys the emerging paradigm of **Agentic RAG**, where **autonomous AI agents** are embedded into the RAG loop to enable dynamic, multi-step retrieval and reasoning ([[2501.09136] Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG](https://ar5iv.org/pdf/2501.09136?ref=hub.athina.ai#:~:text=Agentic%20Retrieval,awareness%20across%20diverse%20applications)). Describes how agents with *â€œagenticâ€ patterns* (reflection, planning, tool use, multi-agent collaboration) can manage retrieval strategies, refine context iteratively, and adapt workflows in real-time ([[2501.09136] Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG](https://ar5iv.org/pdf/2501.09136?ref=hub.athina.ai#:~:text=Agentic%20Retrieval,awareness%20across%20diverse%20applications)). Presents a taxonomy of Agentic RAG architectures and highlights applications in **healthcare, finance, education**, etc., along with implementation challenges ([[2501.09136] Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG](https://ar5iv.org/pdf/2501.09136?ref=hub.athina.ai#:~:text=This%20survey%20provides%20a%20comprehensive,1%7D1%20GitHub%20link)). *Significance:* Extends traditional RAG (which is mostly static) to more **flexible, interactive systems** that can handle complex queries and evolving information needs autonomously. |
| **TrustRAG: Enhancing Robustness and Trustworthiness in RAG**<br>(2025, *Method*) | Proposes **TrustRAG**, a framework to **defend RAG systems from corpus-poisoning attacks** ([[2501.00879] TrustRAG: Enhancing Robustness and Trustworthiness in RAG](https://ar5iv.org/pdf/2501.00879?ref=hub.athina.ai#:~:text=compromised%20and%20irrelevant%20contents%20before,addition%2C%20TrustRAG%20maintains%20high%20contextual)). Uses a **two-stage filtering**: (1) clustering retrieved documents (with cosine similarity + ROUGE) to detect outliers or suspicious inserted content, and (2) a *self-assessment* step that compares the LLMâ€™s internal knowledge with retrieved info to flag inconsistencies ([[2501.00879] TrustRAG: Enhancing Robustness and Trustworthiness in RAG](https://ar5iv.org/pdf/2501.00879?ref=hub.athina.ai#:~:text=compromised%20and%20irrelevant%20contents%20before,addition%2C%20TrustRAG%20maintains%20high%20contextual)). This *plug-and-play, training-free* module can sit atop any LLM. **Results:** Experiments show TrustRAG significantly **improves retrieval accuracy and resilience** under attacks compared to vanilla RAG pipelines ([[2501.00879] TrustRAG: Enhancing Robustness and Trustworthiness in RAG](https://ar5iv.org/pdf/2501.00879?ref=hub.athina.ai#:~:text=extensive%20experimental%20validation%2C%20we%20demonstrate,source%20software%20at)). *Impact:* Increases reliability of RAG in high-stakes applications by preventing malicious or misleading external data from contaminating the generated answers. |
| **Enhancing RAG: A Study of Best Practices**<br>(2025, *Analysis Study*) | A comprehensive empirical study identifying **key design factors for optimal RAG performance** ([[2501.07391] Enhancing Retrieval-Augmented Generation: A Study of Best Practices](https://ar5iv.org/pdf/2501.07391?ref=hub.athina.ai#:~:text=optimal%20performance%20across%20diverse%20applications,Our%20findings%20offer)). The authors implement various advanced RAG designs (e.g. with **query expansion**, different retrieval strategies, and a novel *Contrastive In-Context Learning* technique) and systematically vary parameters: language model size, prompt format, document chunk size, knowledge base scope, retrieval stride, multilingual vs. monolingual knowledge, etc. ([[2501.07391] Enhancing Retrieval-Augmented Generation: A Study of Best Practices](https://ar5iv.org/pdf/2501.07391?ref=hub.athina.ai#:~:text=optimal%20performance%20across%20diverse%20applications,Our%20findings%20offer)). Through extensive experiments, they analyze how each factor impacts answer quality ([[2501.07391] Enhancing Retrieval-Augmented Generation: A Study of Best Practices](https://ar5iv.org/pdf/2501.07391?ref=hub.athina.ai#:~:text=optimal%20performance%20across%20diverse%20applications,Our%20findings%20offer)). *Findings:* Provides **actionable insights** â€“ e.g. how to balance retrieved context breadth vs. relevance for different tasks â€“ to guide practitioners in **tuning RAG systems** for better accuracy and efficiency. |
| **CoRAG: Chain-of-Retrieval Augmented Generation**<br>(2025, *Method*) | Introduces **CoRAG**, an approach where the model **retrieves and reasons in multiple iterative steps** before final answer generation ([[2501.14342] Chain-of-Retrieval Augmented Generation](https://ar5iv.org/pdf/2501.14342?ref=hub.athina.ai#:~:text=This%20paper%20introduces%20an%20approach,augmenting%20existing%20RAG%20datasets%20that)) ([[2501.14342] Chain-of-Retrieval Augmented Generation](https://ar5iv.org/pdf/2501.14342?ref=hub.athina.ai#:~:text=strategies%20to%20scale%20the%20model%E2%80%99s,factual%20and%20grounded%20foundation%20models)). Unlike standard RAG that does one retrieval round, CoRAG dynamically **reformulates queries and performs multi-hop retrieval**, guided by an intermediate chain-of-thought. A novel training scheme uses *rejection sampling* to generate intermediate retrieval chains from only final-answer data, enabling effective multi-step training ([[2501.14342] Chain-of-Retrieval Augmented Generation](https://ar5iv.org/pdf/2501.14342?ref=hub.athina.ai#:~:text=dynamically%20reformulate%20the%20query%20based,benchmark%2C%20CoRAG%20establishes%20a%20new)). **Performance:** CoRAG shows **>10-point Exact Match improvement** on complex multi-hop QA tasks over strong single-retrieval baselines ([[2501.14342] Chain-of-Retrieval Augmented Generation](https://ar5iv.org/pdf/2501.14342?ref=hub.athina.ai#:~:text=strategies%20to%20scale%20the%20model%E2%80%99s,benchmark%2C%20CoRAG%20establishes%20a%20new)). It also achieves **new state-of-the-art on the KILT benchmark** across diverse knowledge-intensive tasks ([[2501.14342] Chain-of-Retrieval Augmented Generation](https://ar5iv.org/pdf/2501.14342?ref=hub.athina.ai#:~:text=benchmarks%20validate%20the%20efficacy%20of,factual%20and%20grounded%20foundation%20models)). *Significance:* Demonstrates that iterative retrieval significantly boosts factual accuracy on complex queries, pointing toward more *reasoning-aware* RAG systems. |
| **FRAMES: Fact, Fetch, and Reason Evaluation**<br>(2024, *Dataset/Benchmark*) | Proposes **FRAMES**, a unified benchmark to evaluate RAG end-to-end on **factuality, retrieval, and reasoning** simultaneously ([[2409.12941] Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation](https://ar5iv.org/pdf/2409.12941?ref=hub.athina.ai#:~:text=end%2C%20we%20propose%20FRAMES%20,LLMs%20struggle%20with%20this%20task)). Consists of challenging multi-hop questions that require combining information from multiple sources ([[2409.12941] Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation](https://ar5iv.org/pdf/2409.12941?ref=hub.athina.ai#:~:text=and%20benchmarks%20to%20evaluate%20these,robust%20and%20capable%20RAG%20systems)). *Findings:* Even state-of-the-art LLMs struggle on FRAMES without retrieval (only ~0.40 accuracy), often due to knowledge gaps ([[2409.12941] Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation](https://ar5iv.org/pdf/2409.12941?ref=hub.athina.ai#:~:text=results%20demonstrating%20that%20even%20state,will%20help%20bridge%20evaluation%20gaps)). But integrating a multi-step retrieval pipeline raises accuracy to **0.66 (a >50% relative improvement) ([[2409.12941] Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation](https://ar5iv.org/pdf/2409.12941?ref=hub.athina.ai#:~:text=results%20demonstrating%20that%20even%20state,robust%20and%20capable%20RAG%20systems))**, demonstrating how crucial retrieval is for complex queries. *Impact:* FRAMES provides a **standardized way to measure holistic RAG performance**, encouraging development of models that fetch relevant facts and reason correctly, rather than evaluating each ability in isolation. |
| **LONGÂ²RAG: Long-Context & Long-Form RAG Benchmark (KPR)**<br>(2024, *Benchmark*) | Addresses evaluation gaps for **long-document retrieval and long-form answer generation** in RAG. **LONGÂ²RAG** (sometimes written â€œLongRAGâ€) is a new test set of 280 questions across 10 domains, each with very lengthy references (avg. 2,444 words per document) ([[2410.23000] \dataset: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall](https://ar5iv.org/pdf/2410.23000?ref=hub.athina.ai#:~:text=exploits%20retrieved%20information,are%20available%20at%20this%20url)). Introduces a novel **Key Point Recall (KPR)** metric that checks how well an LLMâ€™s long-form answer **incorporates key points from the retrieved documents** ([[2410.23000] \dataset: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall](https://ar5iv.org/pdf/2410.23000?ref=hub.athina.ai#:~:text=exploits%20retrieved%20information,are%20available%20at%20this%20url)). *Significance:* Allows researchers to **benchmark RAG systems on long-context scenarios**, ensuring that models can handle extensive information and still produce coherent, well-grounded answers. This is crucial as real-world applications (law, research, finance) often involve long documents. |
| **LSIM (Legal RAG â€“ Logical-Semantic Integration Model)**<br>(2025, *Method*, Legal AI) | Tackles RAG for **legal question-answering**, where pure semantic matching is insufficient. Proposes **LSIM**, a supervised RAG framework that integrates **legal logical structure** ([Elevating Legal LLM Responses: Harnessing Trainable Logical Structures and Semantic Knowledge with Legal Reasoning](https://arxiv.org/html/2502.07912v1#:~:text=but%20existing%20approaches%20typically%20focus,demonstrate%20that%20LSIM%20significantly%20enhances)). It builds a *fact-rule chain* (logical reasoning path) for each question via reinforcement learning, then uses a trainable *Deep Structured Semantic Model* to retrieve relevant prior cases or statutes by combining **semantic similarity + logical match**, and finally uses in-context generation with those documents ([Elevating Legal LLM Responses: Harnessing Trainable Logical Structures and Semantic Knowledge with Legal Reasoning](https://arxiv.org/html/2502.07912v1#:~:text=but%20existing%20approaches%20typically%20focus,demonstrate%20that%20LSIM%20significantly%20enhances)). **Results:** On a real legal QA dataset, LSIM **significantly improves accuracy and reliability** of answers over standard RAG baselines ([Elevating Legal LLM Responses: Harnessing Trainable Logical Structures and Semantic Knowledge with Legal Reasoning](https://arxiv.org/html/2502.07912v1#:~:text=reinforcement%20learning%20predicts%20a%20structured,reliability%20compared%20to%20existing%20methods)), reducing hallucinations and producing more legally grounded advice. *Impact:* Demonstrates RAGâ€™s value in law by injecting domain-specific reasoning, an approach that could generalize to other domains requiring strict logical consistency. |
| **Two-Layer RAG for Medical QA**<br>(2025, *Method/Application*, Medical AI) | Presents a **two-layer RAG architecture** for **clinical question answering**, using **user-generated medical data** (Reddit) to address cliniciansâ€™ queries on emerging drug issues ([Journal of Medical Internet Research - Two-Layer Retrieval-Augmented Generation Framework for Low-Resource Medical Question Answering Using Reddit Data: Proof-of-Concept Study](https://www.jmir.org/2025/1/e66220#:~:text=Objective%3A%20This%20paper%20aims%20to,medical%20information%20on%20social%20media)). The system first retrieves and summarizes relevant posts (layer 1), then aggregates those into a final answer summary (layer 2) ([Journal of Medical Internet Research - Two-Layer Retrieval-Augmented Generation Framework for Low-Resource Medical Question Answering Using Reddit Data: Proof-of-Concept Study](https://www.jmir.org/2025/1/e66220#:~:text=Methods%3A%20We%20proposed%20a%20two,Reddit%20to%20answer%20clinicians%E2%80%99%20questions)). It was tested with a lightweight 7B LLM (Nous-Hermes-2) against GPT-4 on 20 medical queries. **Finding:** The small model with this RAG setup achieved **comparable relevance, coherence, and low hallucination** to GPT-4â€™s answers, with no significant difference on several quality metrics ([Journal of Medical Internet Research - Two-Layer Retrieval-Augmented Generation Framework for Low-Resource Medical Question Answering Using Reddit Data: Proof-of-Concept Study](https://www.jmir.org/2025/1/e66220#:~:text=Results%3A%20Our%20framework%20achieves%20comparable,tailed)). *Significance:* Showcases that RAG enables effective **medical QA in low-resource settings** ([Journal of Medical Internet Research - Two-Layer Retrieval-Augmented Generation Framework for Low-Resource Medical Question Answering Using Reddit Data: Proof-of-Concept Study](https://www.jmir.org/2025/1/e66220#:~:text=Conclusions%3A%20Our%20RAG%20framework%20can,constrained%20settings)), by leveraging external community knowledge. This could help deploy medical assistants where large models or up-to-date expert data are not readily available. |

**Analysis & Trends:** The above papers illustrate several key trends in 2025â€™s RAG research:

- **Enhanced Retrieval Strategies:** Many works aim to improve how relevant information is fetched for generation. For example, *RankRAG* (NeurIPS 2024) showed that instruction-tuning an LLM to **jointly rank contexts and generate answers** can outperform dedicated retrievers, even matching GPT-4 on some benchmarks ([NeurIPS Poster RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs](https://neurips.cc/virtual/2024/poster/95135#:~:text=novel%20method%20called%20RankRAG%2C%20which,outperform%20existing%20expert%20ranking%20models)). Methods like CoRAGâ€™s iterative multi-hop retrieval and LSIMâ€™s logic-based retrieval demonstrate the push towards smarter retrievers that go beyond simple similarity matching.

- **Addressing Hallucination and Accuracy:** A core motivation across these papers is mitigating LLM hallucinations and ensuring factual correctness. Approaches include integrating **structured knowledge (GraphRAG, KG-guided RAG)** to provide reliable facts, using **chain-of-retrieval reasoning (CoRAG)** to verify information step-by-step, and developing benchmarks like FRAMES and LONGÂ²RAG to stress-test factual grounding. **Results** show that incorporating retrieval *drastically improves factual accuracy ([[2409.12941] Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation](https://ar5iv.org/pdf/2409.12941?ref=hub.athina.ai#:~:text=results%20demonstrating%20that%20even%20state,robust%20and%20capable%20RAG%20systems))*, and advanced methods can set new state-of-art performance on knowledge-intensive tasks ([[2501.14342] Chain-of-Retrieval Augmented Generation](https://ar5iv.org/pdf/2501.14342?ref=hub.athina.ai#:~:text=benchmarks%20validate%20the%20efficacy%20of,factual%20and%20grounded%20foundation%20models)).

- **Security and Trustworthiness:** With RAG being used in real-world systems (e.g. search engines, ChatGPT plugins), security is a growing concern. SafeRAG reveals that RAG pipelines are vulnerable to even naive attacks, prompting solutions like TrustRAGâ€™s defensive filtering. Ensuring users can trust RAG outputs â€“ especially in high-stakes fields (medical, legal, finance) â€“ has become **a research priority**, leading to techniques that detect poisoned or irrelevant data before generation ([[2501.00879] TrustRAG: Enhancing Robustness and Trustworthiness in RAG](https://ar5iv.org/pdf/2501.00879?ref=hub.athina.ai#:~:text=compromised%20and%20irrelevant%20contents%20before,addition%2C%20TrustRAG%20maintains%20high%20contextual)) ([[2501.00879] TrustRAG: Enhancing Robustness and Trustworthiness in RAG](https://ar5iv.org/pdf/2501.00879?ref=hub.athina.ai#:~:text=extensive%20experimental%20validation%2C%20we%20demonstrate,source%20software%20at)).

- **Efficiency and Deployment:** Several works (e.g. MiniRAG, two-layer medical RAG) focus on making RAG feasible on **smaller models or devices**. By clever indexing and retrieval (MiniRAGâ€™s graph index) or splitting tasks into subtasks (two-layer RAG), they show that even a 7B parameter model can rival much larger models with the right RAG setup ([[2501.06713] MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation](https://ar5iv.org/pdf/2501.06713?ref=hub.athina.ai#:~:text=efficiency,evaluating%20lightweight%20RAG%20systems%20under)). This democratizes RAG, making it usable in edge devices or situations with limited computing resources.

- **Domain-Specific Adaptations:** The versatility of RAG is evidenced by its adoption in specialized domains. In **healthcare**, RAG is used to pull in the latest medical knowledge or patient-generated data to answer clinical questions  ([Journal of Medical Internet Research - Two-Layer Retrieval-Augmented Generation Framework for Low-Resource Medical Question Answering Using Reddit Data: Proof-of-Concept Study](https://www.jmir.org/2025/1/e66220#:~:text=Objective%3A%20This%20paper%20aims%20to,medical%20information%20on%20social%20media)). In **law**, RAG assists legal reasoning by fetching pertinent statutes/cases and even enforcing logical consistency in answers ([Elevating Legal LLM Responses: Harnessing Trainable Logical Structures and Semantic Knowledge with Legal Reasoning](https://arxiv.org/html/2502.07912v1#:~:text=but%20existing%20approaches%20typically%20focus,demonstrate%20that%20LSIM%20significantly%20enhances)). Finance and customer service are likewise experimenting with RAG-enhanced assistants to stay up-to-date with rapidly changing data ([The State Of Retrieval-Augmented Generation (RAG) In 2025 And ...](https://www.ayadata.ai/the-state-of-retrieval-augmented-generation-rag-in-2025-and-beyond/#:~:text=,enhanced%20AI)). These domain-focused studies underscore that **beyond general QA, RAG is being tailored to niche needs** â€“ often via custom retrieval modules or knowledge integration â€“ and yielding better domain-specific performance than one-size-fits-all LLMs.

- **Surveys and Frameworks:** The emergence of multiple survey papers (GraphRAG, Agentic RAG, etc.) in 2025 indicates that the field is **maturing**. They compile recent innovations into coherent frameworks ([[2501.00309] Retrieval-Augmented Generation with Graphs (GraphRAG)](https://ar5iv.org/pdf/2501.00309?ref=hub.athina.ai#:~:text=Given%20the%20broad%20applicability%2C%20the,Our%20survey%20repository%20is)) ([[2501.09136] Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG](https://ar5iv.org/pdf/2501.09136?ref=hub.athina.ai#:~:text=This%20survey%20provides%20a%20comprehensive,1%7D1%20GitHub%20link)), helping researchers and practitioners understand the landscape (e.g., how to incorporate graphs, or how agent-based RAG differs from traditional RAG). This consolidation of knowledge often correlates with increasing citations, as these surveys become go-to references for new RAG research.

In summary, **2025 has been a landmark year for Retrieval-Augmented Generation research**. We see a strong drive to make LLM-generated content more factual, up-to-date, and trustworthy by intelligently leveraging external information. From enhancing core retrieval mechanisms and adding reasoning steps, to safeguarding against adversarial content and adapting to specialized fields, these works collectively push the performance and reliability of RAG to new heights. Researchers are not only proposing novel models but also providing benchmarks and best-practice guidelines, which will shape the next generation of knowledge-aware AI systems.

